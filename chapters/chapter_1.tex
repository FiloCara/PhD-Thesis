\chapter{Industrial Context and Research Framework}
\minitoc

In this first chapter, we present the context of the Industry 4.0 in which this thesis project is inscribed. Primarily, we will review the industry 4.0 paradigm as well as the benefits it can bring to the manufacturing industry through the application of AI-based data-driven methods. Subsequently we will describe our industrial research framework as well as the min goal of this research work: the quality improvement of the parts produced through the Extrusion Blow-Molding process. Finally, we will present what we consider to be the main contributions of our research work.

\section{Industry 4.0: a promise for improved manufacturing}

Automotive industry is nowadays driven by global competition and the need for fast adaptation of production to the ever-changing market requests. The fourth revolution in industry, Industry 4.0, holds the promise of increased flexibility in manufacturing, along with mass customisation, better quality, and improved productivity \citep{zhong2017intelligent}. As already occurred for the past three revolutions, technical innovations and a new way of perceiving the world, are radically changing the industry. The first industrial revolution at the end of the 18th century introduced steam-powered machines. The second one used electricity to improve productivity and to create mass production. Electronics and information technology, with the introduction of Programmable Logic Controllers (PLC) began the industrial automation and the third industrial revolution. The context of billions of people connected by mobile devices, with unprecedented processing power, storage capacity, and access to knowledge is promoting the emergence of new technologies. Artificial intelligence, robotics, autonomous vehicles, 3-D printing, nanotechnology, biotechnology, materials science, energy storage, and quantum computing are changing the world and today we are on the cusp of the fourth industrial revolution \citep{schwab20164th}.  

The term Industry 4.0 refers to the connection among production departments, tools, machines, “individual things” in general made possible by Internet and CPS (cyber physical systems) \citep{schlapfer2015industry} .
With the digital revolution, the boundaries between the physical and digital worlds are disappearing to create an interconnected factory with strong interactions between employees, machines and products. These connected entities can interact with one another using standard Internet-based protocols and analyse data to predict failure, configure themselves, and adapt to changes.
According to the estimations made by BCG for German companies, Industry 4.0 will have a positive impact on companies with productivity and revenue growth but also on economy with more investments and with an overall six percent increase in employment during the next ten years. Productivity improvements on conversion costs, which exclude the cost of materials, will range from 10 to 20\% in automotive industry, while productivity gains of 5 to 8\% will be achieved if the materials costs are factored in \citep{lorenz2016time}. The revenue growth, as a direct consequence of  Manufacturers demand for enhanced equipment and new data applications, as well as consumer demand for a wider variety of increasingly customised products, is estimated at 30€ billion a year, which is approximately one percent of the German GDP \citep{russmann2015industry}. 

Industry 4.0 is also a new way of looking at performance, with a more precise and immediate vision (based on real-time indicators) of the entire production chain, but also the optimisation of production through the use of artificial intelligence. In the context of an interconnected plant, the large amount of data collected from different sources—production equipment and systems as well as enterprise—can be helpful in taking decision and contributes to a continuous improvement process. In particular, we think that the integration of machine learning models inside our complex industrial processes can reduce the non-quality costs with the increase of the overall equipment effectiveness (OEE). 

In the following subsection we will present what we consider to be the two key elements that have been contributing most to the fourth industrial revolution: the Data and the Machine Learning.


\subsection{Data}

For a long time, information was documented on paper while manufacturing was realised by handicraft, therefore, the integration between information technology and manufacturing technology was neither beneficial nor feasible. Since the advent of the first electronic computer in 1940s, the rapid development of information technology (IT) has been driving manufacturing toward informatization. Since the 1960s, the development of integrated circuits has paved the way for the advancement of computer hardware and software. Since the 1980s, TCP/IP, local area network (LAN), World Wide Web (WWW), and search engine emerged one after another to meet the increasing needs for data storage, indexing, processing, and exchange. All of these information technologies were widely applied in manufacturing. As a result, many advanced manufacturing technologies were put forward, such as computer integrated manufacturing (CIM), computer aided design (CAD), manufacturing execution system (MES), computer aided manufacturing (CAM), enterprise resource planning (ERP), and networked manufacturing (NM), etc. Recently, the rise of New IT technologies such us IoT (Internet of Things) and Cloud solutions continues to provides new sources of data. Due to the deep fusion between IT and manufacturing, the degree of manufacturing smartness is progressively elevated. As a result, the manufacturing data also becomes increasingly richer \citep{tao2018data}.

As a consequence of the multitude of manufacturing technologies, industrial data comes in very different forms. This implies a lot of heterogeneity in the data which tend to complexity any data usage or comparison. Moreover, most of the data available in the manufacturing industry is \textit{Unstructured}. In this thesis we consider as \textit{Structured} any kind of data that can be stored in form of rows and columns in systems like databases or Excel Spreadsheet. Any data that can be stored by respecting this convention, without loosing any information, can be qualified as a structured data. On the other hands, we consider as \textit{Unstructured} any set of data that cannot be stored in a set of rows and columns without losing inner information. Some types of data may be difficult to definitely classify into one or the other category. It could actually depend on the use-case and the data processing objective. For example, an image can be represented as a 2D matrix (for black and white images) or as 3D matrix (for colour images). This representation is a perfectly structured. Therefore an image, or a video, could be considered as a structured data format for someone willing to conduct a spectral analysis, only interested in the pixel values and positions. Nonetheless, the same image can also be considered unstructured if we focus our interest on the content of the image. Indeed, pixel values can not be easily translated into a structured representation of the actual content.
Nonetheless, someone willing to extract design intents from these files will probably argue a different point of view considering the difficulty to access this implicit information. Moreover relying on hypothetical metadata, with varying quality and content, can not be considered as a viable systematic solution to this problem, therefore justifying that many engineering standardised formats could, in fact, be considered as unstructured data formats, under specific objectives, despite being implemented in perfectly structured and standardised data formats.

Unfortunately, dealing with unstructured data is a lot more challenging in a data science perspective \citep{blumberg2003problem}\citep{sagiroglu2013big} \citep{buneman1997adding}. It requires highly complex, expensive and time-consuming feature extraction processes and operations (i.e. a feature represents a descriptor (e.g. colour of a car) in a data science context). It is estimated that the average \textit{Information Systems} (IS) roughly contain around 15\% of structured data and 85\% of unstructured data. Such an assumption seems consistent with the actual status of the manufacturing industry. Furthermore, even if a more optimistic situation is considered, with a balanced rate of 50\% structured and 50\% unstructured data, it still appears critical to be able to mine, explore, exploit, and search in these data. Consequently, we could state that a Big Data context is inherently linked to unstructured data.
Dealing with manufacturing data implies to use and manage large amounts of human-made data. These data come with inherent and recurring issues which highly limit their usability without an extensive pre-processing.

\subsection{Machine Learning and Deep Learning}

As highlighted before in the previous section, the amount of available data is exponentially increasing and thus it can be reasonably considered that humans will not be able any more, in a near future, to process, by hand, these massive amounts of data and perform heavy computations in a parallel manner. Nonetheless, software products based on Machine Learning algorithms, take advantage of parallel computation capabilities and large data quantities to approach human behaviours and understanding in complex situations. Machine Learning, and more generally data-driven methods, introduce a new way to deal with manufacturing problems. Unlike traditional methods of rule-based or mechanism-based modelling, one of the major advantages of data-driven industrial technology is the ability to establish predictive analysis based on insight and evidence contained in the data, which allows for establishing smart management tools for invisible problems and exploring the relationships between complex things. This way, new knowledge is accumulated to form an intelligent system which can be continuously iterated on.

In last decade, the hottest machine learning sub-field, Deep Learning has gained a lot of popularity due to the ability to provide state-of-the-art results in multiple domains: from web searches, to image recognition and detection through Convolutional Neural Networks, to natural language processing with Recurrent Neural Networks and, more recently Self-Attention based Neural Networks. Deep Learning is not a new idea, most of the recent proposed Deep Learning architectures are built using discoveries from the last years of the 20th century. Deep Learning has received new attention again in 2012 when Alex Krizhevsky and his colleagues used deep learning technology \citep{krizhevsky2012imagenet} on ImageNet for the first time to outperform other teams in an image classification task, making people aware of the advantages of deep learning over traditional machine learning, bringing deep learning to the forefront for the first time. The reborn popularity of these computational methods can be attributed to the following reasons:

\begin{itemize}
    \item \emph{Increasing Computer Power}: GPU (Graphical Processing Unit) computing enabled, in the early 2010s, improved calculation performance in the field of Machine Learning. Powerful, fast and cheap GPU-devices are some of the devices which greatly helped researchers to reach performances never achieved before, especially in the Machine Learning (ML) field dedicated to the study of deep Neural Network: Deep Learning (DL). Deep learning involves huge amount of matrix multiplications and other operations which can be massively parallelised and thus sped up on GPUs.
    \item \emph{Larger labeled datasets}: The explosion of Big Data in the last decade, has considerably increased the size of the dataset available inside the manufacturing companies. The availability of an important amount of data is indispensable for the successful application of Deep learning methods because these methods require, in average, more data compared to conventional Machine learning approaches. For instance, the ImageNet dataset \citep{deng2009imagenet} released in 2009, contains more than 14 millions images, with the corresponding labels, which can be used for training image classifiers models.
    \item \emph{Advances in Deep Learning research}: Deep Learning is one of the most popular research topics of the moment and interest in this area is growing every year. As pointed by the "AI index 2019 report" \citep{zhang2021ai}, between 1998 and 2018, the volume of peer-reviewed AI papers has grown by more than 300\%, accounting for 3\% of peer-reviewed journal (Figure \ref{fig:Number of Peer-Reviewed AI Publications}) publications and 9\% of published conference papers. This increase in the number of researchers leads to faster research progression. 
    \item \emph{Open source tools and models}: The democratisation of open sources frameworks such as \textit{PyTorch} \citep{paszke2019pytorch}, \textit{Tensorflow} \citep{tensorflow2015-whitepaper}, \textit{Keras} \citep{chollet2015keras} and \textit{Scikit-Learn} \citep{scikit-learn} allow to apply Machine Learning and Deep learning methods with a few line of code. Moreover, the Machine Learning community is rather open in sharing results. There exist a lot of pre-trained models available online which can be used as a starting point for \textit{Transfer Learning} (\ref{Transfer Learning}).  
\end{itemize}

\begin{figure}
\centerline{\includegraphics[scale=0.9]{images/chapter_1/AI_report.eps}}
\caption{Number of Peer-Reviewed AI Publications, 2000-2019 \citep{zhang2021ai}}
\label{fig:Number of Peer-Reviewed AI Publications}
\end{figure}


\subsubsection{Review of opportunities for the manufacturing industry}

In this section, we presents the result of a literature review we conducted to highlight some of the possibles applications where the Machine Learning could be applied in order to create value for the manufacturing companies. This study should not, in any case, be considered as an exhaustive list of possibilities. Results are summarised in table \ref{tab:ai_benefits}. 

\begin{table}
\label{tab:ai_benefits}
\begin{tabular}{|l|p{6cm}|p{4cm}|}
\hline
%
Domain &
  Benefits &
    Bibliography \\ \hline
% Quality Optimization
Quality Optimisation &
  Decrease the product failure rate at the end of the production line. Optimize key performance index of the final product to meet customer needs. &
    \citep{lieber2013quality, li2018ensemble, chen2008neural, nagorny2017quality, haeussler1996quality} \\ \hline

Maintenance &
  Increase the availability of the production line by preventing the breakdown of equipment in advance. Predict the risk of malfunction of the production line and arrange proactive maintenance. &
    \citep{nguyen2019new, lee2017application, einabadi2019dynamic, li2017intelligent, liu2016prediction}\\ \hline
Fault Diagnosis &
  Prognostic diagnose of production line failure event. Identify the malfunction part of the production line. Predict the abnormal behaviors of machines and equipment. & \citep{toma2020bearing, wong2006modified, chen2014fault, malik2017artificial, arabaci2010automatic} \\ \hline
Scheduling Optimisation &
  Logistic management of the production line, which can maximize the throughput of the production line. Buffer control and product routing management. & \citep{morariu2020machine, woschank2020review, lolli2019machine, zhang2019review, gomes2016developing} \\ \hline
\end{tabular}
\caption{ML opportunities in Manufacturing}
\end{table}

\subsubsection{Challenges of AI in Industry}

Although AI technology has made breakthroughs in many applications, there is still a big gap between large-scale usage in industrial scenarios. This is because industry and manufacturing value stability, standardisation, accuracy, and repeatability, as well as mechanisation, processes, operations, and close integration of process requirements \citep{lee2020industrial}. Before AI technologies can be fully integrated into industrial systems, it is necessary to overcome the challenges of reproducibility, reliability, and security

\paragraph{Reproductibility}

% TO REVIEW

Unfortunately, this practice is not widely adopted by researchers in the manufacturing research
communities. Too few datasets are actually available to conduct research. It can be assumed that
the lack of opened datasets is probably induced by the secretive policies frequently enforced in the manufacturing industry. Unfortunately, this tends to hinder research possibilities and produce the following unwanted side-effects:

\begin{itemize}
    \item Very difficult, if not impossible, to reproduce any claimed result in the state-of-the-art. This highly limits the effectiveness of peer-reviewing.
    \item Very difficult, if not impossible, to compare the proposed approaches to address a specific problem. Aside of qualitative studies, there is no common metric based on a shared dataset to evaluate and rank the different methods.
    \item Developing a dataset may literally cost millions of dollars. If every researcher or research group needs to build a new one from scratch, the developing costs will be are multiplied.
    \item Small research laboratories may not be able to work on some research topics due to a lack of funds available to build a decent dataset. This situation may namely affect the emerging countries.
\end{itemize}



\paragraph{Data issues}


Concerning data, AI technology out the manufacturing industry faces five major challenges and limitations: 

\begin{enumerate}
    \item Training data is heavily dependent on manual work, otherwise it is difficult to obtain a large and comprehensive training data set, and the quality of labeling is heavily dependent on human experience and ability.
    \item The transparency of the model needs
    to be improved since AI algorithms cannot explain how conclusions are reached step-by-step.
    \item Models are not very general, and it is hard to replicate from one application to the next. This means lots of money and energy is needed to train new models for new problems.
    \item The risk of deviation in data and algorithms, much
    like the differences between societies and culture, requires extensive steps to solve.
    \item It is difficult to reach agreement on data privacy and attribution.
\end{enumerate}

\paragraph{Reliability}

% TO REVIEW

According to the different requirements of reliability in different domains and applications, AI can be roughly divided into mission-critical and non-mission-critical applications. Currently, most AI products on the market do not require strict system reliability. As long as a certain threshold of usability is reached, the occasional errors and problems can be tolerated without serious consequences in non-mission-critical applications. For critical applications, if a system has even a small chance of failure, it could lead to serious consequences, causing property loss or even harm to human beings or social stability. This is particularly true for all those applications that have to do with the safety of people. An example of this is the intelligent driving industry: it is expected
that this industry will globally reach \$9.5 billion by 2020. The industry is facing serious challenges in guaranteeing safety. The first fatal drone crash occurred in March of 2018, and an Uber autopilot test car hit and killed a woman in May of 2018. Whether it is true autonomous driving technology or a driving assistance system, its high demand for reliability and intolerance of failures makes it difficult for the technology to truly enter the market before exceeding the human driving level. These challenges are the same in industrial systems. If we want to employ AI technology to control the functioning of an entire system, we must be extremely cautious about mission-critical tasks, which necessitates not just advancements in model and algorithm accuracy, but also security limits and uncertainty management in system design. For instance, for a manufacturing company with low percentage of quality scraps, an AI model with a 80\% accuracy in prediction may lead to incorrect alerts FINIRE 


\section{The research framework: The Extrusion Blow Molding}


The industrial process taken into account for our experimental setting is the Extrusion Blow-Moulding process. Extrusion blow molding is a process used to form hollow thermoplastic objects (especially bottles and containers). The process takes a thin-walled tube called a \textit{parison} that has been formed by extrusion, entraps it between two halves of a larger diameter mold, and then expands it by blowing air into the tube, forcing the parison out against the mold. The outside of the thin-walled part takes the shape of the inside of the mold \citep{poli2001design}.

\begin{figure}
\centerline{\includegraphics[scale=0.75]{images/chapter_1/extrusion_blow_molding.eps}}
\caption{Extrusion Blow-Molding \citep{goodship2015design}}
\label{fig:Extrusion Blow-Molding}
\end{figure}

As easily guessed by the name, the Extrusion Blow-Molding process is composed of two sub-processes: Extrusion and Blow-Molding.

\begin{itemize}
    \item \textit{The Extrusion:} The Extrusion is a continuous-flow process where a plastic material feedstock is fed through a hopper onto a feeding transfer screw. The thermal energy provided by the heating clamps as well as the mechanical energy provided by the screw rotation allow the melting of the plastic material.The die will typically create a tubular extruded cross-section, round or oval depending upon the final shape of the finished blow molded part. The melt is extruded through an annular die of adjustable gap, to form a hollow cylindrical membrane known as a parison. The die gap is the distance between the inner mandrel and the outer bushing. The gap can be varied during the extrusion, due to the tapered nature of the die, by moving either the mandrel or the bushing in a vertical direction. The process of variable die gap extrusion is referred to as parison programming and is utilised to manipulate the thickness distribution in the final part \citep{diraddo1993profile}.
    \item \textit{The Blow-Molding:} Unlike Extrusion, Blow-Molding is a discontinuous-flow process. In extrusion blow moulding, the parison is vertically suspended in the air during which time two mould halves enclose it by the action of a pneumatic or hydraulic mechanism. Internally applied air pressure causes the parison to inflate and take the shape of the inner parts of the molds. Once the blow operation is completed and the part has frozen suitably for ejection, the mold opens and the part is ejected, allowing the extruded parison to be extruded through the mold for the next cycle. 
\end{itemize}

Each phase has parameters that inﬂuence the subsequent phases and, ultimately, the characteristics of the ﬁnished product. The multiple phases mean that the number of parameters that can be adjusted on the process is fairly high. It is possible to fine-tune temperatures, screw speeds and throughputs for the Extrusion, as well as the  pressure curves, molds opening and closing times for the Blow-Molding phase. In addition, the Extrusion Blow-moulding process has a certain dynamic: it takes a certain amount of time for the adjustment of one of the process parameters to have an effect on the products. This dynamic is mainly due to the thermal inertia of the solid tooling.
One of the most critical part of the process is the parison formation. In fact, the dimensions of the blow molded article are directly related to the dimensions of the parison. Furthermore, the thermo-mechanical history of the material during the parison formation stage and the resulting weight and diameter distribution of the parison have a great influence on the characteristics of the subsequent inflation and cooling stages. The shape and the dimensions of the parison are the result of complex interactions between the molten ploymer and the thermo-mechanical conditions that influence the melt after it leaves the extruder die. Parison formation it is affected by two phenomena knows as \textit{swell} and \textit{sag}. Parison swell, occurring both in diameter and thickness, is due to the nonlinear viscoelastic deformation of the polymer melt in the extrusion die. Sag is caused by gravitational forces that act on the suspended parison \citep{huang2002prediction}. A high degree of swell could lead to molded articles which are to heavy and uneconomical, on the other hands, very low swelling could yield incomplete parts of low weight with unacceptable wall thickness.

There are many variations of this process including equipment made to extrude multiple parisons simultaneously, equipment that can extrude multiple layers within the same parison, equipment with rotary capability that will hold several molds and can provide a continuous nonstop process. Moreover, the extrusion blow-molding process can be split into two subcategories: intermittent extrusion blow molding and continuous extrusion blow molding. With intermittent extrusion blow molding, the extruder runs for a designated amount of time and fills a reservoir with plastic. Once the the reservoir has been filled, a plunger is activated and pushes the material from the reservoir through the extrusion head. On the other hand, in continuous blow-molding, the plastic is extruded permanently in a continuous manner while the machine runs.

In the context of this thesis project we have mainly worked with a continuous Extrusion Blow-Molding process, whose finished products are obtained from the overlay of multiple plastic layers. This particular type of Blow-Molding process takes the name of \textit{Co-extrusion}. Co-extrusion was born out of the need of reduce the permeability of fuel tanks. The steps for producing a multi-layers plastic product are the same as those used by the traditional single layer process, except for the number of extruders involved in the manufacturing process. Up to six extruders can be used simultaneously to melt different plastic materials. The goal is to create a multi-layer tank with the use of different material, as shown in
Figure \ref{fig:Co-extrusion Process}

\begin{figure}
\centerline{\includegraphics[scale=0.55]{images/chapter_1/coextrusion.png}}
\caption{Co-extrusion Process}
\label{fig:Co-extrusion Process}
\end{figure}

The Extrusion Blow-Molding constitutes one of the various stages necessary to produce the finished part. The stages needed to manufacture a finished part may vary depending on the type of product made. For instance, a fuel tank container, a plastic bottle or a plastic bumper may requires different post-production processes. Further information on how a fuel tank is produced are available in Appendix \ref{Full production process}. As far as our thesis work is concerned, we will only focus on The Extrusion Blow-Molding stage.


\subsection{The key parameters of the Extrusion Blow-Molding} \label{The key parameters of the Extrusion Blow-Molding}

The following table resume the list of the key parameters of an extrusion blow-molding process.

% Put table in landscape view
\begin{landscape}
\begin{table}[]
\caption{Blow-Molding Key parameters}
\label{tab:key_parameters}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Process      & Parameter            & Unity of measure & Value range   & Description                 & Dependencies \\ \hline
Extrusion    & Speed*               & RPM              & {[}0, 90{]}   & Rotation speed of the screw &              \\ \hline
Extrusion    & Throughput*          & Kg/h             &               &                             &              \\ \hline
Extrusion    & Feeding Temperature* & °C               &               &                             &              \\ \hline
Extrusion    & Melt Temperature*    & °C               &               &                             &              \\ \hline
Extrusion    & Cycle time           & s                & {[}60, 120{]} &                             &              \\ \hline
Extrusion    & Parison profile      &                  &               &                             &              \\ \hline
Blow-Molding & Blowing pressure**   &                  &               &                             &              \\ \hline
Blow-Molding &                      &                  &               &                             &              \\ \hline
Blow-Molding &                      &                  &               &                             &              \\ \hline
\end{tabular}
\end{table}
\end{landscape}

\subsection{The Quality of an extruded blow-molded part}

Quality Control is generally expressed as a management activity verifying the conformity of the process and the product/service to the requirements that constitute its quality standard. The ISO 9000 standard defines quality control as "A part of quality management focused on fulfilling quality requirements". In the industrial context taken into account the requirements are defined by the customers. Several characteristics are measured on the product and compared with the customer requirements, if the measures are compliant with the customer specifications the part can be sent to the customer, otherwise the part have to be rejected. 
In the framework of the Extrusion Blow-Molding process, we are mostly interested in the dimensional/geometric characteristics. In fact, 


The following table resume some of the specific tests that have to be carried to the blow-molded parts to ensure their conformity to the customer specifications.


\begin{table}[]

\caption{Quality tests}
\end{table}


% Insert Blow-Molding table

\section{Main objective}

Because manufacturing processes are becoming more and more complex, and the high level of requirement in the automotive industry regarding safety and environmental impacts, Plastic Omnium is continuously seeking for innovation throughout its different projects that allows the company to remain leader in its field. For Plastic Omnium, the Industry 4.0 paradigm can provide a new way of looking at performance, with a more precise and immediate vision (based on real-time indicators) of the entire production chain, but also the optimisation of production through the use of data-driven methods. An in-depth presentation of the activities of Plastic Omnium is available in Appendix \ref{Plastic Omnium}.

For Plastic Omnium the are four main pillars of the 



\begin{figure}
\centerline{\includegraphics[scale=0.50]{images/chapter_1/Digitalisation_pillars.png}}
\caption{Industry 4.0 pillars for Plastic Omnium}
\label{fig:pillars}
\end{figure}


Among the different topics, this research work will focus on the Quality topic.
For an equipment manufacturer like Plastic Omnium Clean Energy Systems, bad or ``scrap'' parts are very expensive for the company. The “Cost of Non-Quality” (CNQ) is one of the key indicators most used by the company. 



Using the data that is already available within the company will be an important part of the global study, because it will be the first input going into the developed monitoring system. In the case of Plastic Omnium CES, the data coming from the systems "PES" and "DASIP" will be important. The two systems allow the traceability of the produced parts as well as a monitoring of the different events happening on the plant’s machines (for PES), and DASIP is monitoring key parameters of the production processes. Other data sources will be investigated during this work.

To reach the final objective of reducing the scraps and the non-quality costs, a methodology will be proposed in order to implement a predictive system. This methodology will include both the requirements to solve the industrial and scientific issues discussed in the next part. A phase of validation and performance analysis of the proposed approach is also required, to ensure its relevance in the industrial context, and its interest compared to other approaches of the literature.

Previous works focused mainly on two themes: Process optimisation and Quality monitoring.  


\section{Contributions: Towards new quality control framework}

Our dissertation develops along two major axes: the process improvement and the quality control improvement. We claims that the improving of the overall quality of a production line can be obtained by working either on the process and either on the quality control. This for us definitely makes sense as the production of a part compliant with the specifications is the result of a careful work in optimising the production process, as well as the ability to quickly identify a deviation in the quality of the finished part. By quickly identifying a quality problem it is possible to react faster and adjust the process, limiting the production of non-conforming parts.

This thesis has, in our opinion three main contributions:

\begin{enumerate}
    \item It provides a general framework to improve the Process Control of a complex manufacturing process. We propose a data-driven approach which take advantage of the historical data to try to model the relationship between process parameters and the quality of a product. The approach presented in chapters 3 aims to leverage the information available in the large amount of data collected in the manufacturing process 
    
    Our approach, presented in Chapter 3 is composed of 4 main stages: the data acquisition, the data processing, the Machine Learning modeling and the FINIRE. The four stages 
    \item It provides a general framework to improve the Quality Control. Using a data-driven approach we can use sensors, cameras or any kind of equipment to collect meaningful data. This approach open ups new possibilities in term of Quality Control.  
    \item In Chapter 4, we propose an approach to infer the thickness of Blow-Molded parts using thermal imaging and a Deep Learning without any direct measurement of the part. We claims that this    
\end{enumerate}


\section{Conclusion}

In this first chapter we have highlighted the context in wh

\subsection{Scientific Contribution}

\subsection{Industrial Contribution}

In this chapter we have seen that we are on the cusp of the 4th industrial resolution. Industry 4.0, holds the promise of increased flexibility in manufacturing, along with mass customization, better quality, and improved productivity. The development of new technologies such us the AI, the IoT are opening up new perspective in the Manufacturing Industry. AI in particular seems able to take advantage of the fast growing amount of data available in the manufacturing plants to FINIRE 
